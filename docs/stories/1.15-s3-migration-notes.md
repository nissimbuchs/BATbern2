# Story 1.15 Implementation Notes: S3 Content Migration Strategy

**Note:** These implementation notes should be integrated into Story 1.15 when it is drafted.

## Content Migration to S3 (AC11)

**Architecture Reference:** [PRD Section 4.2 - Content Management & Storage Architecture](../prd-enhanced.md#42-content-management--storage-architecture), [Infrastructure Doc S3 Section](../architecture/02-infrastructure-deployment.md#aws-s3-content-storage-infrastructure), [Data Architecture Content Metadata](../architecture/03-data-architecture.md#content-storage--file-management)

### Migration Strategy Overview

The historical BATbern data includes 20+ years of presentations, speaker photos, company logos, and event images that must be migrated from the existing Angular application to AWS S3 with proper metadata tracking.

### Content Inventory

**Source Data:**
- **Presentations**: 54+ events Ã— avg 12 sessions = ~650 PDF/PPTX files
- **Speaker Photos**: ~400 speaker profiles with profile images
- **Company Logos**: ~80 partner companies with logo files
- **Event Photos**: Photo galleries from 54+ events
- **File Size Estimate**: 15-20 GB total content

**Source Locations:**
- Existing Angular app file system or cloud storage
- Legacy JSON data with file paths/references
- Image CDN URLs (if applicable)

### S3 Migration Implementation

#### Step 1: Pre-Migration Analysis

```java
@Service
public class ContentMigrationAnalysisService {

    public MigrationAnalysisReport analyzeContentForMigration() {
        // Scan legacy data sources
        List<LegacyFile> presentations = scanLegacyPresentations();
        List<LegacyFile> speakerPhotos = scanLegacySpeakerPhotos();
        List<LegacyFile> companyLogos = scanLegacyCompanyLogos();
        List<LegacyFile> eventPhotos = scanLegacyEventPhotos();

        return MigrationAnalysisReport.builder()
            .totalFiles(presentations.size() + speakerPhotos.size() +
                       companyLogos.size() + eventPhotos.size())
            .totalSizeBytes(calculateTotalSize(...))
            .filesByType(Map.of(
                ContentType.PRESENTATION, presentations.size(),
                ContentType.SPEAKER_PHOTO, speakerPhotos.size(),
                ContentType.LOGO, companyLogos.size(),
                ContentType.EVENT_PHOTO, eventPhotos.size()
            ))
            .estimatedMigrationTime(calculateEstimatedTime(...))
            .warnings(identifyPotentialIssues(...))
            .build();
    }
}
```

#### Step 2: Batch Migration with Spring Batch

```java
@Configuration
public class ContentMigrationBatchConfig {

    @Bean
    public Job contentMigrationJob() {
        return jobBuilderFactory.get("contentMigrationJob")
            .start(migratePresentationsStep())
            .next(migrateSpeakerPhotosStep())
            .next(migrateCompanyLogosStep())
            .next(migrateEventPhotosStep())
            .next(validateMigrationStep())
            .build();
    }

    @Bean
    public Step migratePresentationsStep() {
        return stepBuilderFactory.get("migratePresentations")
            .<LegacyFile, ContentMigrationResult>chunk(10)
            .reader(legacyPresentationReader())
            .processor(s3MigrationProcessor())
            .writer(contentMetadataWriter())
            .faultTolerant()
            .retry(IOException.class)
            .retryLimit(3)
            .listener(migrationProgressListener())
            .build();
    }
}
```

#### Step 3: S3 Upload with Metadata Tracking

```java
@Service
public class S3MigrationProcessor implements ItemProcessor<LegacyFile, ContentMigrationResult> {

    private final S3Client s3Client;
    private final ContentMetadataRepository metadataRepository;

    @Override
    public ContentMigrationResult process(LegacyFile legacyFile) throws Exception {
        try {
            // Read legacy file content
            byte[] fileContent = readLegacyFile(legacyFile);

            // Generate S3 key following new convention
            String s3Key = generateS3Key(legacyFile);
            String bucket = getBucketForContentType(legacyFile.getContentType());

            // Upload to S3
            PutObjectRequest putRequest = PutObjectRequest.builder()
                .bucket(bucket)
                .key(s3Key)
                .contentType(legacyFile.getMimeType())
                .metadata(Map.of(
                    "legacy-id", legacyFile.getLegacyId(),
                    "original-path", legacyFile.getOriginalPath(),
                    "migration-date", Instant.now().toString()
                ))
                .build();

            s3Client.putObject(putRequest, RequestBody.fromBytes(fileContent));

            // Create content metadata record
            ContentMetadata metadata = new ContentMetadata();
            metadata.setFileId(UUID.randomUUID().toString());
            metadata.setS3Bucket(bucket);
            metadata.setS3Key(s3Key);
            metadata.setOriginalFilename(legacyFile.getFilename());
            metadata.setFileSizeBytes((long) fileContent.length);
            metadata.setMimeType(legacyFile.getMimeType());
            metadata.setChecksum(calculateSHA256(fileContent));
            metadata.setContentType(legacyFile.getContentType());
            metadata.setUploadStatus(UploadStatus.COMPLETED);
            metadata.setUploadedBy("MIGRATION_SYSTEM");
            metadata.setUploadedAt(Instant.now());
            metadata.setCloudFrontUrl(generateCloudFrontUrl(s3Key));
            metadata.setMetadata(Map.of("legacy-id", legacyFile.getLegacyId()));

            metadataRepository.save(metadata);

            return ContentMigrationResult.success(legacyFile, metadata);

        } catch (Exception e) {
            log.error("Failed to migrate file: {}", legacyFile.getOriginalPath(), e);
            return ContentMigrationResult.failure(legacyFile, e.getMessage());
        }
    }

    private String generateS3Key(LegacyFile legacyFile) {
        // Follow S3 key convention: /{content-type}/{year}/{entity-id}/{filename-with-uuid}.{ext}
        int year = extractYearFromLegacyData(legacyFile);
        String entityId = extractEntityId(legacyFile); // event-id, speaker-id, company-id
        String uuid = UUID.randomUUID().toString().substring(0, 8);
        String extension = getFileExtension(legacyFile.getFilename());

        return String.format("%s/%d/%s/%s-%s%s",
            legacyFile.getContentType().getFolderName(),
            year,
            entityId,
            sanitizeFilename(legacyFile.getFilename()),
            uuid,
            extension
        );
    }
}
```

#### Step 4: Update Entity References

```java
@Service
public class EntityReferenceMigrationService {

    public void updateSessionWithPresentationFile(UUID sessionId, ContentMetadata migratedFile) {
        // Create presentation_files record linking session to migrated S3 file
        PresentationFile presentationFile = new PresentationFile();
        presentationFile.setSessionId(sessionId);
        presentationFile.setFileId(migratedFile.getFileId());
        presentationFile.setTitle(extractTitleFromLegacy(migratedFile));
        presentationFile.setIsPrimary(true);
        presentationFile.setUploadedBy("MIGRATION_SYSTEM");
        presentationFile.setUploadedAt(migratedFile.getUploadedAt());
        presentationFile.setDownloadCount(0);
        presentationFile.setFileUrl(migratedFile.getCloudFrontUrl());

        presentationFileRepository.save(presentationFile);
    }

    public void updateCompanyWithLogo(UUID companyId, ContentMetadata migratedLogo) {
        Company company = companyRepository.findById(companyId)
            .orElseThrow(() -> new CompanyNotFoundException(companyId));

        company.setLogoUrl(migratedLogo.getCloudFrontUrl());
        company.setLogoS3Key(migratedLogo.getS3Key());
        company.setLogoFileId(migratedLogo.getFileId());

        companyRepository.save(company);
    }
}
```

### Migration Performance Optimization

**Parallel Processing:**
- Use Spring Batch partitioning for parallel file uploads
- Partition by content type (4 parallel partitions: presentations, photos, logos, event photos)
- Chunk size: 10 files per batch for optimal S3 throughput

**Network Optimization:**
- Use AWS SDK multipart upload for files > 10 MB
- Enable connection pooling for S3 client
- Run migration from EC2 instance in same region as S3 buckets for faster transfer

**Progress Tracking:**
```java
@Component
public class MigrationProgressListener implements StepExecutionListener {

    @Override
    public void beforeStep(StepExecution stepExecution) {
        log.info("Starting migration step: {}", stepExecution.getStepName());
    }

    @Override
    public ExitStatus afterStep(StepExecution stepExecution) {
        long processed = stepExecution.getReadCount();
        long failed = stepExecution.getWriteSkipCount();
        long duration = stepExecution.getEndTime().getTime() -
                       stepExecution.getStartTime().getTime();

        log.info("Migration step {} completed: {} processed, {} failed, duration: {}ms",
            stepExecution.getStepName(), processed, failed, duration);

        return stepExecution.getExitStatus();
    }
}
```

### Data Validation and Integrity Checks

**Post-Migration Validation:**
```java
@Service
public class ContentMigrationValidationService {

    public ValidationReport validateMigration() {
        // Count files in legacy system
        long legacyFileCount = countLegacyFiles();

        // Count files in S3/database
        long migratedFileCount = contentMetadataRepository.countByUploadedBy("MIGRATION_SYSTEM");

        // Verify checksums for sample files
        List<ContentMetadata> sampleFiles = contentMetadataRepository.findRandomSample(100);
        List<ValidationError> checksumErrors = verifySampleChecksums(sampleFiles);

        // Check for orphaned S3 objects
        List<String> orphanedS3Keys = findOrphanedS3Objects();

        // Check for missing references
        List<String> missingReferences = findMissingEntityReferences();

        return ValidationReport.builder()
            .legacyFileCount(legacyFileCount)
            .migratedFileCount(migratedFileCount)
            .successRate((double) migratedFileCount / legacyFileCount * 100)
            .checksumErrors(checksumErrors)
            .orphanedFiles(orphanedS3Keys)
            .missingReferences(missingReferences)
            .isValid(checksumErrors.isEmpty() && orphanedS3Keys.isEmpty())
            .build();
    }
}
```

### Rollback Strategy

**Rollback Implementation:**
```java
@Service
public class ContentMigrationRollbackService {

    public void rollbackMigration(LocalDate migrationDate) {
        // Find all content migrated on specified date
        List<ContentMetadata> migratedContent = contentMetadataRepository
            .findByUploadedByAndCreatedAtAfter("MIGRATION_SYSTEM",
                migrationDate.atStartOfDay().toInstant(ZoneOffset.UTC));

        log.info("Rolling back {} migrated files", migratedContent.size());

        for (ContentMetadata metadata : migratedContent) {
            try {
                // Delete from S3
                s3Client.deleteObject(DeleteObjectRequest.builder()
                    .bucket(metadata.getS3Bucket())
                    .key(metadata.getS3Key())
                    .build());

                // Delete metadata record
                contentMetadataRepository.delete(metadata);

                // Remove entity references (presentation_files, company logos, etc.)
                removeEntityReferences(metadata);

                log.info("Rolled back file: {}", metadata.getOriginalFilename());

            } catch (Exception e) {
                log.error("Failed to rollback file: {}", metadata.getFileId(), e);
            }
        }
    }
}
```

### Migration Monitoring Dashboard

**Real-time Progress Tracking:**
- Total files to migrate
- Files successfully migrated
- Files failed (with error details)
- Current migration speed (files/sec, MB/sec)
- Estimated time remaining
- S3 storage used
- Data integrity validation status

### Performance Benchmarks

**Target Metrics:**
- **Total Migration Time**: < 4 hours (per Epic 1 Definition of Done)
- **Upload Speed**: 10 MB/s average (per infrastructure capacity)
- **Parallel Workers**: 4 concurrent batch partitions
- **Batch Size**: 10 files per chunk
- **Success Rate**: > 99.9% (failures < 0.1%)
- **Data Integrity**: 100% checksum validation on sample

### Test Considerations

**Integration Tests:**
- Mock legacy file system with test data
- Use LocalStack S3 for S3 operations
- Test batch job with small dataset (10-20 files)
- Verify content metadata creation
- Verify entity reference updates
- Test rollback procedure

**Manual Testing:**
- Run migration on staging environment with production-like data
- Verify CloudFront CDN URLs work
- Test file downloads through new platform
- Compare file checksums between legacy and migrated files
- Verify all entity references (sessions, companies, speakers) are correctly linked

---

## Integration with Story 1.15

When drafting Story 1.15, incorporate this S3 migration strategy into:

1. **Acceptance Criteria AC11** - Expand "File Migration" criterion with S3-specific implementation details
2. **Test Specifications** - Add Spring Batch test scenarios for S3 migration
3. **Tasks/Subtasks** - Add specific tasks for S3 upload, metadata creation, validation
4. **Architecture Context** - Reference the content storage architecture (PRD 4.2, Infrastructure 02)

**Key Architecture References:**
- [PRD Section 4.2 - Content Management & Storage Architecture](../prd-enhanced.md#42-content-management--storage-architecture)
- [Infrastructure S3 Architecture](../architecture/02-infrastructure-deployment.md#aws-s3-content-storage-infrastructure)
- [Data Architecture Content Metadata](../architecture/03-data-architecture.md#content-storage--file-management)
- [API Design File Endpoints](../architecture/04-api-design.md#file-storage--content-management)
